{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDGvGSCqOvhDboh5QmW4gX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"QqxXCnbG_Btt"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import os\n","import tempfile\n","import pickle\n","\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Patch\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import sklearn\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","\n","from keras.utils.np_utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.metrics import Recall, Precision"]},{"cell_type":"code","source":["## reference : https://gmnam.tistory.com/230#:~:text=class%20BlockingTimeSeriesSplit%28%29%3A%20def%20__init__%28self%2C%20n_splits%29%3A%20self.n_splits%20%3D%20n_splits,indices%20%5Bstart%3A%20mid%5D%2C%20indices%20%5Bmid%20%2B%20margin%3A%20stop%5D\n","cmap_data = plt.cm.Paired\n","cmap_cv = plt.cm.coolwarm\n","plt.style.use('fivethirtyeight')\n","\n","class BlockingTimeSeriesSplit():\n","    def __init__(self, n_splits):\n","        self.n_splits = n_splits\n","\n","    def get_n_splits(self, groups):\n","        return self.n_splits\n","\n","    def split(self, X, y=None, groups=None):\n","        n_samples = len(X)\n","        k_fold_size = n_samples // self.n_splits\n","        indices = np.arange(n_samples)\n","\n","        margin = 0\n","        for i in range(self.n_splits):\n","            start = i * k_fold_size\n","            stop = start + k_fold_size\n","            mid = int(0.8 * (stop - start)) + start\n","            yield indices[start: mid], indices[mid + margin: stop]\n","\n","def plot_cv_indices(cv, X, n_splits, lw=10):\n","\n","    fig, ax = plt.subplots()\n","    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n","\n","    # Generate the training/testing visualizations for each CV split\n","    for ii, (tr, tt) in enumerate(cv.split(X=X)):\n","        # Fill in indices with the training/test groups\n","        indices = np.array([np.nan] * len(X))\n","        indices[tt] = 1\n","        indices[tr] = 0\n","\n","        # Visualize the results\n","        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n","                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n","                   vmin=-.2, vmax=1.2)\n","\n","    # Formatting\n","    yticklabels = list(range(n_splits))\n","    ax.set(yticks=np.arange(n_splits) + .5, yticklabels=yticklabels,\n","           xlabel='Sample index', ylabel=\"CV iteration\",\n","           ylim=[n_splits+0.1, -.1], xlim=[0, len(X)])\n","    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n","\n","    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n","          ['Testing set', 'Training set'], loc=(1.02, .8))"],"metadata":{"id":"08BUvCLZ_LcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## time_step means the length(n) of explanatory variables(t-(n-1) ~ t) to use for classifying y in certain time(t) period.\n","def create_dataset(X, y, time_steps=1):\n","    Xs, ys = [], []\n","\n","    if type(Xs)==pd.core.frame.DataFrame:\n","      for i in range(len(X) - time_steps):\n","          v = X.iloc[i:(i + time_steps)].values\n","          Xs.append(v)\n","          ys.append(y.iloc[i + time_steps].values)\n","          Xs=np.array(Xs)\n","          ys=np.array(ys)\n","\n","    else:\n","      for i in range(len(X) - time_steps):\n","          v = X[i:(i + time_steps)]\n","          Xs.append(v)\n","          ys.append(y[i + time_steps])\n","\n","    return np.array(Xs), np.array(ys)"],"metadata":{"id":"oYyEpLAG_Vr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## make index for train, validation dataset\n","def make_split(X_data, n_splits, test_year):\n","  train_idxs=[]\n","  val_idxs=[]\n","\n","  tss=BlockingTimeSeriesSplit(n_splits=n_splits)\n","\n","  for train_idx, val_idx in tss.split(X_data[:-(test_year*12)]):\n","    train_idxs.append(train_idx)\n","    val_idxs.append(val_idx)\n","\n","  return train_idxs, val_idxs\n"],"metadata":{"id":"AQQz_KGN_cEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_model(num_layer, X_train, nc, metrics, loss_type):\n","    model = Sequential()\n","    model.add(LSTM(num_layer, input_shape=(X_train.shape[1], X_train.shape[2])))  ##input_shape=(X_train.shape[1], X_train.shape[2])\n","    # model.add(Dropout(0.2))\n","    #model.add(Dense(2))\n","    model.add(Dense(nc, activation='sigmoid'))\n","    model.compile(loss=loss_type, optimizer='adam', metrics=metrics)  ##['AUC','Accuracy','Recall','Precision','F1Score']\n","    return model"],"metadata":{"id":"rE8a6v-x_dmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def grid_search(path, X_data, y_data, y_type, n_splits, test_year, metrics=['AUC','Accuracy','Recall','Precision'],\n","                nl=[10, 30, 50, 100], epochs=[30,50,70,100], batch_sizes=[6], time_steps=[12,18,24]):\n","  results = []\n","  history_dict={}\n","  best_auc = 0\n","  best_params = None\n","  nc=len(y_data.unique())\n","\n","  train_idxs, val_idxs = make_split(X_data, n_splits, test_year)\n","\n","  for time_step in time_steps:\n","    loss_type = 'categorical_crossentropy'   ##if y_type == 'y_agg' else 'binary_crossentropy'\n","    for num_epochs in epochs:\n","      for batch_size in batch_sizes:\n","        for num_layer in nl:\n","\n","          auc=0\n","          acc=0\n","          recall=0\n","          precision=0\n","\n","          for train_idx, val_idx in zip(train_idxs, val_idxs):\n","\n","            X_train=X_data.iloc[list(train_idx)]\n","            y_train=y_data.iloc[list(train_idx)]\n","            X_val=X_data.iloc[list(val_idx)]\n","            y_val=y_data.iloc[list(val_idx)]\n","\n","\n","            X_test=X_data.iloc[-test_year*12:]\n","            y_test=y_data.iloc[-test_year*12:]\n","\n","            ##standard scaling\n","            scaler = StandardScaler()\n","            trainx = scaler.fit_transform(X_train)\n","            valx = scaler.transform(X_val)\n","            testx = scaler.transform(X_test)\n","\n","            ##make timestep\n","            trainx, trainy = create_dataset(X_train, y_train, time_step)\n","            valx, valy = create_dataset(X_val, y_val, time_step)\n","            testx, testy = create_dataset(X_test, y_test, time_step)\n","            trainy = to_categorical(trainy, num_classes=nc)\n","            valy = to_categorical(valy, num_classes=nc)\n","            testy = to_categorical(testy, num_classes=nc)\n","\n","            ##assign class weight(more weight for less frequent class)\n","            neg, pos = np.bincount(y_train)\n","            total = neg + pos\n","\n","            weight_for_0 = (1 / neg) * (total / 2.0)\n","            weight_for_1 = (1 / pos) * (total / 2.0)\n","\n","            class_weight = {0: weight_for_0, 1: weight_for_1}\n","\n","\n","            # create the model weighted more for class 1\n","            model=make_model(num_layer, trainx, nc, metrics, loss_type)\n","            history=model.fit(trainx, trainy, epochs=num_epochs, batch_size=batch_size, class_weight=class_weight)\n","#                  print(model.summary())\n","\n","            # Final evaluation of the model\n","            scores = model.evaluate(valx, valy, verbose=0)\n","            #print(scores)\n","            auc += scores[1]\n","            acc += scores[2]\n","            recall += scores[3]\n","            precision += scores[4]\n","            #print(\"AUC: %.2f%%\" % (auc))\n","\n","          auc=auc/n_splits\n","          acc=acc/n_splits\n","          recall=recall/n_splits\n","          precision=precision/n_splits\n","\n","          final_scores=model.evaluate(testx, testy, verbose=0)  ##test...?\n","          final_auc=final_scores[1]\n","          final_acc=final_scores[2]\n","          final_recall=final_scores[3]\n","          final_precision=final_scores[4]\n","\n","          model_name='clf_lstm_{}_{}_b{}_ep{}_h{}'.format(y_type, time_step, batch_size, num_epochs, num_layer)\n","          model.save(path+'/model/{}.h5'.format(model_name))\n","          current_result = {'model':model_name,\n","                            'auc': auc,\n","                            'accuracy': acc,\n","                            'recall': recall,\n","                            'precision': precision,  #\"%.2f%%\" % (precision)\n","                            'final_auc':final_auc,\n","                            'final_acc':final_acc,\n","                            'final_recall':final_recall,\n","                            'final_precision':final_precision,\n","                            }\n","          results.append(current_result)\n","\n","          if auc > best_auc:\n","            best_model = model_name\n","            best_auc = auc\n","            best_params = current_result\n","\n","  print(\"Best model: \", best_model)\n","  return results\n"],"metadata":{"id":"XfpyrHgm_ejp"},"execution_count":null,"outputs":[]}]}